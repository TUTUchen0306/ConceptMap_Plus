{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from flask import Flask, send_file, send_from_directory, request, Response\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import subprocess\n",
    "import firebase_admin\n",
    "from firebase_admin import db\n",
    "import download \n",
    "import datetime\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "ROOT = \"vis_json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/evelyn/Desktop/ConceptMap/ConceptGuide_code-main/MyResearchCode',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.10/lib/python310.zip',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/opennre-0.1-py3.10.egg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/evelyn/Desktop/ConceptMap/ConceptGuide_code-main/MyResearchCode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = 'natural_language_processing_introduction_50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if concept == \"natural_language_processing_introduction_50\":\n",
    "    f = open('/Users/evelyn/Desktop/ConceptMap/ConceptGuide_code-main/MyResearchCode/vis_json/NLP_intro.json','r')\n",
    "\n",
    "elif concept == \"bitcoin_introduction_50\":\n",
    "    f = open('/Users/evelyn/Desktop/ConceptMap/ConceptGuide_code-main/MyResearchCode/vis_json/bitcoin_introduction.json','r')\n",
    "\n",
    "elif concept == \"bitcoin_mining_50\":\n",
    "    f = open('/Users/evelyn/Desktop/ConceptMap/ConceptGuide_code-main/MyResearchCode/vis_json/bitcoin_mining.json','r')\n",
    "\n",
    "\n",
    "original = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_name_index_match = {} #concept name-index dict\n",
    "for concept in original['concept_relationship']['nodes']:\n",
    "    existing_name_index_match[concept['name']] = concept['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BFSinput', 'RemovedCycleLinks', 'VideoSequence_ConceptInfo', 'concept_relationship', 'concept_sequences', 'highlight_nodes', 'search_info', 'video_sequences', 'videos_info'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/evelyn/Desktop/ConceptMap/ConceptGuide_code-main/MyResearchCode\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prerequisite': None,\n",
       " 'similarity': 0.9999999999999999,\n",
       " 'source': 0,\n",
       " 'target': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original['concept_relationship']['links'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'5isBs5WFbf0': [10],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'n25JjoixM3I': [0, 11, 10]},\n",
       " '1': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 27, 0, 25],\n",
       "  'n25JjoixM3I': [0, 11, 25, 24, 10, 27],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24]},\n",
       " '2': {'5isBs5WFbf0': [10],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'n25JjoixM3I': [0, 11, 2, 10]},\n",
       " '3': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 3, 27, 0, 25],\n",
       "  'n25JjoixM3I': [0, 11, 25, 24, 10, 27],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24]},\n",
       " '4': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 27, 0, 25],\n",
       "  'fOvTtapxa9c': [11, 4, 10, 24, 1, 0],\n",
       "  'n25JjoixM3I': [0, 11, 25, 4, 24, 10, 27],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24, 4]},\n",
       " '5': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'MNvT5JekDpg': [5, 1, 27, 10, 11, 24, 0, 25],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 27, 0, 25],\n",
       "  'n25JjoixM3I': [0, 11, 25, 24, 10, 27, 5],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24]},\n",
       " '6': {},\n",
       " '7': {},\n",
       " '8': {},\n",
       " '9': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 15, 27, 0, 25],\n",
       "  'Y90BJzUcqlI': [10, 11, 9, 25, 0],\n",
       "  'fOvTtapxa9c': [20, 11, 10, 15, 24, 1, 0],\n",
       "  'n25JjoixM3I': [0, 11, 25, 24, 10, 27],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24]},\n",
       " '10': {},\n",
       " '11': {},\n",
       " '12': {},\n",
       " '13': {'d4gGtcobq8M': [13, 12]},\n",
       " '14': {},\n",
       " '15': {'5isBs5WFbf0': [10],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [15, 0],\n",
       "  'n25JjoixM3I': [0, 11, 10]},\n",
       " '16': {},\n",
       " '17': {},\n",
       " '18': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'FLZvOKSCkxY': [0, 18, 11, 10, 1, 25, 27],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 27, 0, 25],\n",
       "  'n25JjoixM3I': [0, 11, 25, 24, 10, 27],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24]},\n",
       " '19': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'MNvT5JekDpg': [5, 1, 27, 10, 11, 24, 0, 25],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 27, 0, 25],\n",
       "  'fOvTtapxa9c': [11, 4, 10, 24, 1, 0],\n",
       "  'n25JjoixM3I': [0, 11, 19, 25, 4, 24, 10, 27, 5, 7],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24, 4],\n",
       "  'yKN8a8jgIN8': [7, 1, 19]},\n",
       " '20': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 15, 27, 0, 25],\n",
       "  'fOvTtapxa9c': [20, 11, 10, 15, 24, 1, 0],\n",
       "  'n25JjoixM3I': [0, 11, 25, 24, 10, 27],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24]},\n",
       " '21': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'MNvT5JekDpg': [21, 1, 27, 10, 11, 24, 0, 25],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 15, 27, 0, 21, 25],\n",
       "  'n25JjoixM3I': [0, 11, 25, 24, 10, 27],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24]},\n",
       " '22': {'FLZvOKSCkxY': [22, 11], 'NT40U8zU1bg': [11, 8, 22]},\n",
       " '23': {},\n",
       " '24': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 27, 0, 25],\n",
       "  'n25JjoixM3I': [0, 11, 25, 24, 10, 27],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24]},\n",
       " '25': {'5isBs5WFbf0': [10, 25],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'n25JjoixM3I': [0, 11, 25, 10]},\n",
       " '26': {},\n",
       " '27': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 27, 0, 25],\n",
       "  'n25JjoixM3I': [0, 11, 25, 24, 10, 27],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24]},\n",
       " '28': {'5isBs5WFbf0': [10, 25, 27],\n",
       "  'MNvT5JekDpg': [21, 28, 1, 27, 10, 11, 24, 0, 25],\n",
       "  'NT40U8zU1bg': [11, 0],\n",
       "  'Sx3Fpw0XCXk': [24, 15, 27, 0, 21, 25],\n",
       "  'fOvTtapxa9c': [20, 11, 10, 15, 24, 1, 0],\n",
       "  'n25JjoixM3I': [0, 11, 25, 24, 10, 27],\n",
       "  'w9OUpjiu_zg': [27, 1, 11, 25, 24]},\n",
       " '29': {'w9OUpjiu_zg': [16, 29]}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original['VideoSequence_ConceptInfo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Generate a sample review for video\n",
    "\"Natural Language Processing In 10 Minutes | NLP Tutorial For Beginners | NLP Training | Edureka\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplereview1 = \"\"\"NLP is used to help understanding natural language, which's unstructured for insight. The technique is applied in different domains such as sentiment analysis, chatbot, search engine, etc. The steps included are very complicated, like path of speech, Speech recognition, and NER. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplereview2 = \"\"\"The natural message is complex and diverse but textual form is highly unstructured in nature. So NLP usually involves the process of structuring the input text deriving patterns within the structured data and finally evaluating and interpreting the output.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplereview3 = 'Natural language processing is all about making computers to learn, process and manipulate natural speech.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplereview_csv = pd.read_csv('/Users/evelyn/Desktop/ConceptMap/experimental/results/bert_1_pred_NLP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: get YAKE keywords and other potential keywords using different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq_list = pd.read_csv(\"/Users/evelyn/Desktop/ConceptMap/ConceptGuide_code-main/MyResearchCode/unigram_freq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocess_utils module centralises all text cleaning processes\n",
    "# https://github.com/jackragless/GLOGEN-automatic-GLOssary-GENerator\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import re\n",
    "#nltk.download('wordnet',quiet=True)\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "\n",
    "\n",
    "def remove_grammar(text_to_clean):\n",
    "    final = ''\n",
    "    for i in text_to_clean:\n",
    "        if i.isalpha() or i.isdigit() or i=='-' or i==' ' or i=='.':\n",
    "            final += i\n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_bracket_content(text_to_clean):\n",
    "    return re.sub(\"[\\[\\(].*?[\\]\\)]\", \"\", text_to_clean)\n",
    "\n",
    "\n",
    "\n",
    "def fix_nospace_sents(text_to_clean):\n",
    "    final = text_to_clean\n",
    "    for i in range(len(final)-2):\n",
    "        if (final[i].isalpha() and final[i].islower()) and final[i+1] == '.' and (final[i+2].isalpha() and final[i+2].isupper()):\n",
    "            final = final[:i+2] + ' ' + final[i+2:]\n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_settings():\n",
    "    print(\"\"\"\n",
    "        1) remove_bracket_content_bool\n",
    "        2) remove_grammar_bool\n",
    "        3) remove_stopword_bool\n",
    "        4) lemmatize_bool\n",
    "        5) lowercase_bool\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text_to_clean, remove_bracket_content_bool, remove_grammar_bool, remove_stopword_bool, lemmatize_bool, lowercase_bool):\n",
    "\n",
    "    text_to_clean = text_to_clean.replace('\\n','').replace('\"',\"'\").replace('=','')\n",
    "    \n",
    "    text_to_clean = fix_nospace_sents(text_to_clean)\n",
    "\n",
    "    \n",
    "    if remove_bracket_content_bool == True:\n",
    "        text_to_clean = remove_bracket_content(text_to_clean)\n",
    "    \n",
    "    if remove_grammar_bool == True:\n",
    "        text_to_clean = remove_grammar(text_to_clean)\n",
    "        \n",
    "    clean_sent_arr = []\n",
    "        \n",
    "    for sent in nltk.sent_tokenize(text_to_clean):\n",
    "        \n",
    "        temp_sent = []\n",
    "        \n",
    "        if lowercase_bool == True:\n",
    "            sent = sent.lower()\n",
    "        \n",
    "        if remove_stopword_bool == True and lemmatize_bool == True:\n",
    "            for word in nltk.word_tokenize(sent):   \n",
    "                if word.lower() not in stop_words:\n",
    "                    temp_sent.append(lemmatizer.lemmatize(word))\n",
    "                    \n",
    "        if remove_stopword_bool == False and lemmatize_bool == False:\n",
    "            for word in nltk.word_tokenize(sent):   \n",
    "                    temp_sent.append(word)\n",
    "\n",
    "        elif remove_stopword_bool == False and lemmatize_bool == True:\n",
    "            for word in nltk.word_tokenize(sent):   \n",
    "                    temp_sent.append(lemmatizer.lemmatize(word))\n",
    "                    \n",
    "        elif remove_stopword_bool == True and lemmatize_bool == False:\n",
    "            for word in nltk.word_tokenize(sent):   \n",
    "                if word.lower() not in stop_words:\n",
    "                    temp_sent.append(word) \n",
    "                    \n",
    "        clean_sent_arr.append(TreebankWordDetokenizer().detokenize(temp_sent))\n",
    "        \n",
    "    final_clean_text = ' '.join(clean_sent_arr)\n",
    "    final_clean_text = re.sub(' +', ' ', final_clean_text).strip()\n",
    "\n",
    "    return final_clean_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pos_tag(text): \n",
    "    for_tagging = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        if sent[-1] == '.':\n",
    "            for_tagging.append(nltk.word_tokenize(sent[:-1]))\n",
    "        else:\n",
    "            for_tagging.append(nltk.word_tokenize(sent))\n",
    "\n",
    "    tagged = []\n",
    "    sent_num = 0\n",
    "    for sent in for_tagging:\n",
    "        sent_num += 1\n",
    "        for word in nltk.pos_tag(sent):\n",
    "            if word[0].lower() not in stop_words:\n",
    "                word = list(word)\n",
    "                word.insert(0,sent_num)\n",
    "                tagged.append(word)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yake\n",
    "#from keybert import KeyBERT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from itertools import combinations\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "#from nltk.corpus import wordnet\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PhraseReduction(phrase):  #e.g. \"sorting algorithm\"=>\"sort algorithm\" #noun phrase\n",
    "    strr=\"\"\n",
    "    vlist=phrase.split(' ')\n",
    "    for voc in vlist:\n",
    "        for word, pos in pos_tag(word_tokenize(voc)):\n",
    "            try:\n",
    "                wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "                strr = strr+lemmatizer.lemmatize(word, pos=wordnet_pos)+\" \"\n",
    "            except:\n",
    "                strr = strr+word+\" \"\n",
    "    #print(\"phraseReduction\",phrase,strr[:-1])\n",
    "    return strr[:-1]\n",
    "\n",
    "\n",
    "def removeSubstring(vlist):  # e.g. for two word phrase, if both \"bubble\" and \"sort\" and \"bubble sort\" exist in Top30 Words,remove\"bubble\" \"sort\"\n",
    "    checklist = vlist[:]\n",
    "    \n",
    "    \n",
    "    for voctuple in vlist:\n",
    "        splitlist = voctuple.split(' ')\n",
    "        if(len(splitlist)>1):\n",
    "            #print(splitlist)\n",
    "            remove_words = []\n",
    "            for item in splitlist:\n",
    "                t = [t for t in checklist if PhraseReduction(t)==PhraseReduction(item)]\n",
    "                if(len(t)!=0):\n",
    "                    remove_words.append(t[0])\n",
    "                #print(remove_words)\n",
    "            \n",
    "            if(len(remove_words)==len(splitlist)): # all words have single voc corresponding\n",
    "                for word in remove_words:\n",
    "                    if word in vlist:\n",
    "                        vlist.remove(word)\n",
    "                        print(\"c_remove \",word)\n",
    "    return vlist\n",
    "\n",
    "\n",
    "def updateWlist(klist): # from keywords.py in other_modules # update\n",
    "    # ori_list = list(ori_set)\n",
    "    klistset = set(klist)\n",
    "    common_sw = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gon na\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n",
    "    common_words_fromfreq = list(unigram_freq_list[:500]['word']) # remove 500 common words that are most frequent.\n",
    "    stop_list_special=['part','of','for','back','it','good morning','to','at','in','the','video','key','simple','bitcoin bitcoin','with','we','do','time','and','make','[ music ]','day','best','work','be','is','are','by','sex','set','meet tomorrow','language processing','natural language','good thing','next','hello', 'gon na','people','person', 'human', 'humanbeing']\n",
    "    stop_list = list(set(common_sw+stop_list_special+common_words_fromfreq))\n",
    "    \n",
    "    #remove stopwords\n",
    "    klistset = [i for i in klistset if i not in stop_list]    \n",
    "    \n",
    "    # substring processing\n",
    "    klistset = removeSubstring(klistset)\n",
    "    \n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pstemmer = PorterStemmer()\n",
    "    #todo: stem the keywords and simplify them even further\n",
    "    #      but the original keyword list need to save so that they could be used for highlight and relation extraction.\n",
    "    #      mapping between stemed word and original keyword list\n",
    "    klistset = set([lemmatizer.lemmatize(i) for i in klistset])\n",
    "     \n",
    "    \n",
    "    return list(klistset)\n",
    "\n",
    "\n",
    "def keyword_update_yake(text_summary, filter_method = 'filter_by_size', numOfKeywords=10):\n",
    "    \"\"\"\n",
    "    filter_method: filter_by_score or filter_by_size, default is filter_by_score\n",
    "    \"\"\"\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_thresold = 0.2\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 2\n",
    "    #numOfKeywords = \n",
    "    \n",
    "    cleaned_text_summary = text_summary#clean_text(text_summary, True, False, True, False, False); decision: didn't remove stop words and lemme, YAKE will also do this part.\n",
    "    #print(cleaned_text_summary)\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "    new_keyword_candidates = custom_kw_extractor.extract_keywords(cleaned_text_summary)\n",
    "    new_keyword_candidates.sort(key = lambda x: x[1])\n",
    "    print(new_keyword_candidates)\n",
    "\n",
    "    \n",
    "    \n",
    "    #top concept selection\n",
    "    #way 1: filter by keyword score\n",
    "    if filter_method == 'filter_by_score':\n",
    "        top_n_filter = 0.1\n",
    "        new_kw_list = filter(lambda keyword_tuple: keyword_tuple[1] < top_n_filter, new_keyword_candidates)\n",
    "        print(new_kw_list)\n",
    "    #way 2: filter by new keyword size, percentage of all words in summary.\n",
    "    elif filter_method == 'filter_by_size':\n",
    "        flatten_list = ' '.join(text_summary)#[item for sublist in doc_lists for item in sublist]\n",
    "        n_cad = int(np.log(round(len(flatten_list)))*2)\n",
    "        print(n_cad)\n",
    "        print(np.log(round(len(flatten_list))))\n",
    "        new_kw_list = new_keyword_candidates[:n_cad]\n",
    "    else:\n",
    "        new_kw_list = []\n",
    "        print(\"there is no new keywords found this round. Plese re-try!\")\n",
    "        sys.exit()\n",
    "\n",
    "    new_kw_list_words = [keyword_tuple[0] for keyword_tuple in new_kw_list]\n",
    "\n",
    "    return new_kw_list_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('understanding natural language', 0.05176296201968456), ('NLP', 0.13074998523904527), ('speech', 0.14203733858119666), ('insight', 0.143824021024546), ('technique is applied', 0.22013292927812678), ('Natural language processing', 0.26890917315418217), ('chatbot', 0.3087625994566953), ('NLP usually involves', 0.33884278949613467), ('domains', 0.42475542292782753), ('computers to learn', 0.5461357764012915)]\n",
      "14\n",
      "7.160069207596127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['understanding natural language',\n",
       " 'NLP',\n",
       " 'speech',\n",
       " 'insight',\n",
       " 'technique is applied',\n",
       " 'Natural language processing',\n",
       " 'chatbot',\n",
       " 'NLP usually involves',\n",
       " 'domains',\n",
       " 'computers to learn']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yake_keyword_candidates = keyword_update_yake(' '.join([samplereview1,samplereview2,samplereview3]))\n",
    "yake_keyword_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('video', 0.00447446780062828), ('great', 0.006035213877238886), ('NLP', 0.013105047649878476), ('Hey', 0.013824713712582903), ('awesome', 0.017196708159034856), ('error', 0.024042480951110237), ('good', 0.02675577124032728), ('man', 0.032594203643043315), ('sentiment analysis', 0.050610270410989866), ('python', 0.05515884608108267)]\n",
      "21\n",
      "10.878481060664019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['video',\n",
       " 'great',\n",
       " 'NLP',\n",
       " 'Hey',\n",
       " 'awesome',\n",
       " 'error',\n",
       " 'good',\n",
       " 'man',\n",
       " 'sentiment analysis',\n",
       " 'python']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yake_keyword_candidates = keyword_update_yake(' '.join(samplereview_csv['textDisplay'].to_string(index=False).split()))\n",
    "yake_keyword_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wikipedia links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "def scrapeWikiArticle(url):\n",
    "    response = requests.get(url=url)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    title = soup.find(id=\"firstHeading\")\n",
    "    print(title.text)\n",
    "\n",
    "    allLinks = soup.find(id=\"bodyContent\").find_all(\"a\", href=True)\n",
    "    allContent = []\n",
    "    \n",
    "    for link in allLinks:\n",
    "        linkattr = link.attrs\n",
    "        if ('href' in linkattr) and ('title' in linkattr):\n",
    "            if \"/wiki/\" in linkattr['href']:\n",
    "                #print(linkattr['title'])\n",
    "                link_title = linkattr['title']\n",
    "                \n",
    "                # remove content from bracket\n",
    "                clean_link_title = re.sub(r\"\\([^()]*\\)\", \"\", link_title)\n",
    "                allContent.append(clean_link_title)\n",
    "            \n",
    "        #if (link['href'].find(\"/wiki/\") != -1) and (link['title']):\n",
    "        #    print(link['title'])\n",
    "        #print(\"\\n\")\n",
    "    \n",
    "    #todo: need to remove links from references\n",
    "    \n",
    "#     allContent = []\n",
    "#     for link in allLinks:\n",
    "        \n",
    "#         if (link['href'].find(\"/wiki/\") != -1):\n",
    "            \n",
    "#             #print(type(link.contents[0]))\n",
    "#             if len(link.contents) > 0:\n",
    "#                 if link.contents[0].name:\n",
    "#                     #print(link.contents[0].name)\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     allContent.append(link.contents[0].lower())\n",
    "\n",
    "    #for link in allLinks:\n",
    "        # We are only interested in other wiki articles\n",
    "    #    if link['href'].find(\"/wiki/\") == -1: \n",
    "    #        continue\n",
    "\n",
    "        # Use this link to scrape\n",
    "    #    linkToScrape = link\n",
    "    #    break\n",
    "\n",
    "    #scrapeWikiArticle(\"https://en.wikipedia.org\" + linkToScrape['href'])\n",
    "    \n",
    "    return allContent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_keyword_candidates(learning_topic, learning_topic_category):\n",
    "    \n",
    "    # part 1.2: extract keywords from wikipedia\n",
    "    wikipedia_page_links = wikipedia.page(learning_topic).links\n",
    "    category_articles = scrapeWikiArticle(wikipedia.page(learning_topic_category).url)\n",
    "    wikipedia_keywords = set([i.lower() for i in wikipedia_page_links + category_articles])\n",
    "    common_wikipedia_keywords = set(wikipedia_page_links).intersection(category_articles)\n",
    "    \n",
    "    return wikipedia_keywords, common_wikipedia_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# card_conceptrelationship_nodes(original['concept_relationship']['nodes'], original['videos_info'], ' '.join(post_list), existing_index_max, keyword_algo = 'keyword_update_yake') \n",
    "\n",
    "# get concepts from the reviews, and only from the reviews.\n",
    "\n",
    "def review_conceptrelationship_nodes(concepts_resource, video_resource, text, existing_concepts_nameindex_dict, wikipedia_keywords, keyword_algo = 'keyword_update_yake'):\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # part1.1: extract keywords from review using YAKE\n",
    "    if keyword_algo == 'keyword_update_yake':\n",
    "        yake_keyword_candidates = keyword_update_yake(text.text)\n",
    "    ic(yake_keyword_candidates)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # part 1.3: filter keywrods from review and wikipedia\n",
    "    keyword_reviews = []\n",
    "    for chunk in text.noun_chunks:\n",
    "        #print(chunk.text)\n",
    "        if chunk.text.lower() in yake_keyword_candidates:\n",
    "            print(chunk.text, ', YES in YAKE algorithm')\n",
    "            if chunk.text not in keyword_reviews:\n",
    "                keyword_reviews.append(chunk.text)\n",
    "        elif chunk.text.lower() in wikipedia_keywords:\n",
    "            print(chunk.text, ', YES in Wikipedia')\n",
    "            if chunk.text not in keyword_reviews:\n",
    "                keyword_reviews.append(chunk.text)\n",
    "    \n",
    "    \n",
    "    # part 1.4: add concepts if from existing concepts as well but not from 1.1, 1.2, 1.3\n",
    "    old_concepts_ifpossible = [ i['name'] for i in concepts_resource if i['name'] in text.text.lower()]\n",
    "    \n",
    "    new_concepts_list_final = updateWlist([key.lower() for key in keyword_reviews] + list(old_concepts_ifpossible)) #updateWlist(keyword_reviews+old_concepts_ifpossible)\n",
    "    ic(new_concepts_list_final)\n",
    "\n",
    "    # part 2: fill up more concept details with 'conceptCountForEachVid', 'videos_id'\n",
    "    new_concepts_toremove = new_concepts_list_final[:] #avoid shallow copy\n",
    "    videovid_l = video_resource.keys()\n",
    "    new_concepts_resource = []\n",
    "    for old_concept_res in concepts_resource:\n",
    "        if old_concept_res['name'] in new_concepts_list_final:\n",
    "            old_concept_res['group'] = 3 # 3 means it shows up in both original map and new reviews\n",
    "            new_concepts_resource.append(old_concept_res)\n",
    "            new_concepts_toremove.remove(old_concept_res['name'])\n",
    "\n",
    "    \n",
    "    #existing_concepts_index_max = max(existing_concepts_index)\n",
    "    if len(new_concepts_toremove)>0:\n",
    "        for new_concept in new_concepts_toremove:\n",
    "            print(new_concept)\n",
    "            new_res_temp = dict()\n",
    "            new_res_temp['name'] = new_concept.lower()\n",
    "            new_res_temp['group'] = 2 # 2 means it show up in reviews\n",
    "            ic(max(existing_concepts_nameindex_dict.values()))\n",
    "            new_res_temp['index'] = max(existing_concepts_nameindex_dict.values()) + 1\n",
    "            # the existing_concepts_nameindex_dict will update in the global variable as well\n",
    "            existing_concepts_nameindex_dict[new_res_temp['name']] = new_res_temp['index']\n",
    "            \n",
    "            \n",
    "            conceptcount_temp = dict()\n",
    "            for videoid in videovid_l:\n",
    "                concept_count_1v = video_resource[videoid]['transcript'].count(new_concept)\n",
    "                if  concept_count_1v > 0:\n",
    "                    conceptcount_temp[videoid] = concept_count_1v\n",
    "            new_res_temp['conceptCountForEachVid'] = conceptcount_temp\n",
    "            new_res_temp['count'] = sum(conceptcount_temp.values())\n",
    "            \n",
    "            conceptvideos_id = sorted(list(conceptcount_temp.items()), key=lambda k: k[1], reverse=True) \n",
    "            if(len(conceptvideos_id) > 6):\n",
    "                conceptvideos_id = conceptvideos_id[:6]\n",
    "            new_res_temp['videos_id'] = conceptvideos_id\n",
    "            \n",
    "            print(new_res_temp)\n",
    "            new_concepts_resource.append(new_res_temp)\n",
    "  \n",
    "            \n",
    "    return new_concepts_resource # related_concept_resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_reviews = nlp_spacy(' '.join([samplereview1,samplereview2,samplereview3]) )\n",
    "#wikipedia_supplementary_spacy = nlp_spacy(wikipedia_supplementary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_reviews = nlp_spacy(' '.join(samplereview_csv['textDisplay'].to_string(index=False).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:Computational linguistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| yake_keyword_candidates: ['video',\n",
      "                              'great',\n",
      "                              'NLP',\n",
      "                              'Hey',\n",
      "                              'awesome',\n",
      "                              'error',\n",
      "                              'good',\n",
      "                              'man',\n",
      "                              'sentiment analysis',\n",
      "                              'python']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('video', 0.00447446780062828), ('great', 0.006035213877238886), ('NLP', 0.013105047649878476), ('Hey', 0.013824713712582903), ('awesome', 0.017196708159034856), ('error', 0.024042480951110237), ('good', 0.02675577124032728), ('man', 0.032594203643043315), ('sentiment analysis', 0.050610270410989866), ('python', 0.05515884608108267)]\n",
      "21\n",
      "10.878481060664019\n",
      "man , YES in YAKE algorithm\n",
      "python , YES in YAKE algorithm\n",
      "python , YES in YAKE algorithm\n",
      "python , YES in YAKE algorithm\n",
      "video , YES in YAKE algorithm\n",
      "Sentiment Analysis , YES in YAKE algorithm\n",
      "sentiment analysis , YES in YAKE algorithm\n",
      "Awesome , YES in YAKE algorithm\n",
      "Sentiment Analysis , YES in YAKE algorithm\n",
      "Python , YES in YAKE algorithm\n",
      "video , YES in YAKE algorithm\n",
      "python , YES in YAKE algorithm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| new_concepts_list_final: ['python',\n",
      "                              'awesome',\n",
      "                              'machine learning',\n",
      "                              'google wave',\n",
      "                              'watson',\n",
      "                              'similarity',\n",
      "                              'language model',\n",
      "                              'nlp',\n",
      "                              'english language',\n",
      "                              'understand',\n",
      "                              'machine',\n",
      "                              'computer science',\n",
      "                              'google',\n",
      "                              'sentiment analysis']\n",
      "ic| max(existing_concepts_nameindex_dict.values()): 32\n",
      "ic| max(existing_concepts_nameindex_dict.values()): 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "{'name': 'python', 'group': 2, 'index': 33, 'conceptCountForEachVid': {'FLZvOKSCkxY': 3}, 'count': 3, 'videos_id': [('FLZvOKSCkxY', 3)]}\n",
      "awesome\n",
      "{'name': 'awesome', 'group': 2, 'index': 34, 'conceptCountForEachVid': {'FLZvOKSCkxY': 2, 'MNvT5JekDpg': 3, 'Y90BJzUcqlI': 1}, 'count': 6, 'videos_id': [('MNvT5JekDpg', 3), ('FLZvOKSCkxY', 2), ('Y90BJzUcqlI', 1)]}\n"
     ]
    }
   ],
   "source": [
    "keyword_candidates_fromwiki = wikipedia_keyword_candidates( 'Natural language processing', 'Category:Computational linguistics')\n",
    "\n",
    "review_nodes = review_conceptrelationship_nodes(original['concept_relationship']['nodes'], original['videos_info'], doc_reviews, existing_name_index_match, keyword_candidates_fromwiki) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'conceptCountForEachVid': {'4vLlF3flSeg': 3,\n",
       "   'FLZvOKSCkxY': 1,\n",
       "   'LAtzapS2GBU': 1,\n",
       "   'MC97BZqdq0w': 3,\n",
       "   'MNvT5JekDpg': 5,\n",
       "   'fOvTtapxa9c': 2,\n",
       "   'w9OUpjiu_zg': 13,\n",
       "   'yKN8a8jgIN8': 1},\n",
       "  'count': 29,\n",
       "  'group': 3,\n",
       "  'index': 1,\n",
       "  'name': 'machine learning',\n",
       "  'videos_id': [['w9OUpjiu_zg', 13],\n",
       "   ['MNvT5JekDpg', 5],\n",
       "   ['4vLlF3flSeg', 3],\n",
       "   ['MC97BZqdq0w', 3],\n",
       "   ['fOvTtapxa9c', 2],\n",
       "   ['LAtzapS2GBU', 1]]},\n",
       " {'conceptCountForEachVid': {'Sx3Fpw0XCXk': 3},\n",
       "  'count': 3,\n",
       "  'group': 3,\n",
       "  'index': 3,\n",
       "  'name': 'google wave',\n",
       "  'videos_id': [['Sx3Fpw0XCXk', 3]]},\n",
       " {'conceptCountForEachVid': {'MC97BZqdq0w': 1,\n",
       "   'fOvTtapxa9c': 3,\n",
       "   'n25JjoixM3I': 2,\n",
       "   'w9OUpjiu_zg': 1},\n",
       "  'count': 7,\n",
       "  'group': 3,\n",
       "  'index': 4,\n",
       "  'name': 'computer science',\n",
       "  'videos_id': [['fOvTtapxa9c', 3],\n",
       "   ['n25JjoixM3I', 2],\n",
       "   ['w9OUpjiu_zg', 1],\n",
       "   ['MC97BZqdq0w', 1]]},\n",
       " {'conceptCountForEachVid': {'n25JjoixM3I': 1, 'yKN8a8jgIN8': 8},\n",
       "  'count': 9,\n",
       "  'group': 3,\n",
       "  'index': 7,\n",
       "  'name': 'watson',\n",
       "  'videos_id': [['yKN8a8jgIN8', 8], ['n25JjoixM3I', 1]]},\n",
       " {'conceptCountForEachVid': {'GDso6md3DBw': 23, 'Y90BJzUcqlI': 1},\n",
       "  'count': 24,\n",
       "  'group': 3,\n",
       "  'index': 9,\n",
       "  'name': 'similarity',\n",
       "  'videos_id': [['GDso6md3DBw', 23], ['Y90BJzUcqlI', 1]]},\n",
       " {'conceptCountForEachVid': {'3Q8wacwA4gs': 1,\n",
       "   '4vLlF3flSeg': 5,\n",
       "   '5isBs5WFbf0': 9,\n",
       "   'FLZvOKSCkxY': 1,\n",
       "   'LAtzapS2GBU': 4,\n",
       "   'MNvT5JekDpg': 4,\n",
       "   'Y90BJzUcqlI': 3,\n",
       "   '_RY1QUXjV10': 3,\n",
       "   'd4gGtcobq8M': 4,\n",
       "   'fOvTtapxa9c': 2,\n",
       "   'n25JjoixM3I': 1,\n",
       "   'uCUdlM8KnPk': 2},\n",
       "  'count': 39,\n",
       "  'group': 3,\n",
       "  'index': 10,\n",
       "  'name': 'nlp',\n",
       "  'videos_id': [['5isBs5WFbf0', 9],\n",
       "   ['4vLlF3flSeg', 5],\n",
       "   ['LAtzapS2GBU', 4],\n",
       "   ['MNvT5JekDpg', 4],\n",
       "   ['d4gGtcobq8M', 4],\n",
       "   ['_RY1QUXjV10', 3]]},\n",
       " {'conceptCountForEachVid': {'3Q8wacwA4gs': 2,\n",
       "   '4vLlF3flSeg': 7,\n",
       "   'FLZvOKSCkxY': 4,\n",
       "   'GDso6md3DBw': 1,\n",
       "   'MC97BZqdq0w': 1,\n",
       "   'MNvT5JekDpg': 3,\n",
       "   'NT40U8zU1bg': 8,\n",
       "   'Y90BJzUcqlI': 3,\n",
       "   'd4gGtcobq8M': 1,\n",
       "   'fOvTtapxa9c': 4,\n",
       "   'n25JjoixM3I': 8,\n",
       "   'uCUdlM8KnPk': 2,\n",
       "   'w9OUpjiu_zg': 4},\n",
       "  'count': 48,\n",
       "  'group': 3,\n",
       "  'index': 11,\n",
       "  'name': 'understand',\n",
       "  'videos_id': [['NT40U8zU1bg', 8],\n",
       "   ['n25JjoixM3I', 8],\n",
       "   ['4vLlF3flSeg', 7],\n",
       "   ['w9OUpjiu_zg', 4],\n",
       "   ['FLZvOKSCkxY', 4],\n",
       "   ['fOvTtapxa9c', 4]]},\n",
       " {'conceptCountForEachVid': {'Sx3Fpw0XCXk': 3, 'fOvTtapxa9c': 2},\n",
       "  'count': 5,\n",
       "  'group': 3,\n",
       "  'index': 15,\n",
       "  'name': 'language model',\n",
       "  'videos_id': [['Sx3Fpw0XCXk', 3], ['fOvTtapxa9c', 2]]},\n",
       " {'conceptCountForEachVid': {'3Q8wacwA4gs': 1,\n",
       "   '4vLlF3flSeg': 2,\n",
       "   'FLZvOKSCkxY': 4},\n",
       "  'count': 7,\n",
       "  'group': 3,\n",
       "  'index': 18,\n",
       "  'name': 'sentiment analysis',\n",
       "  'videos_id': [['FLZvOKSCkxY', 4], ['4vLlF3flSeg', 2], ['3Q8wacwA4gs', 1]]},\n",
       " {'conceptCountForEachVid': {'4vLlF3flSeg': 1,\n",
       "   'FLZvOKSCkxY': 5,\n",
       "   'NT40U8zU1bg': 1},\n",
       "  'count': 7,\n",
       "  'group': 3,\n",
       "  'index': 22,\n",
       "  'name': 'english language',\n",
       "  'videos_id': [['FLZvOKSCkxY', 5], ['4vLlF3flSeg', 1], ['NT40U8zU1bg', 1]]},\n",
       " {'conceptCountForEachVid': {'4vLlF3flSeg': 7,\n",
       "   'LAtzapS2GBU': 1,\n",
       "   'MNvT5JekDpg': 3,\n",
       "   'Sx3Fpw0XCXk': 5,\n",
       "   'fOvTtapxa9c': 2,\n",
       "   'n25JjoixM3I': 2,\n",
       "   'w9OUpjiu_zg': 1},\n",
       "  'count': 21,\n",
       "  'group': 3,\n",
       "  'index': 24,\n",
       "  'name': 'google',\n",
       "  'videos_id': [['4vLlF3flSeg', 7],\n",
       "   ['Sx3Fpw0XCXk', 5],\n",
       "   ['MNvT5JekDpg', 3],\n",
       "   ['n25JjoixM3I', 2],\n",
       "   ['fOvTtapxa9c', 2],\n",
       "   ['w9OUpjiu_zg', 1]]},\n",
       " {'conceptCountForEachVid': {'3Q8wacwA4gs': 1,\n",
       "   '5isBs5WFbf0': 1,\n",
       "   'FLZvOKSCkxY': 1,\n",
       "   'LAtzapS2GBU': 1,\n",
       "   'MNvT5JekDpg': 5,\n",
       "   'Sx3Fpw0XCXk': 2,\n",
       "   'n25JjoixM3I': 1,\n",
       "   'w9OUpjiu_zg': 13},\n",
       "  'count': 25,\n",
       "  'group': 3,\n",
       "  'index': 27,\n",
       "  'name': 'machine',\n",
       "  'videos_id': [['w9OUpjiu_zg', 13],\n",
       "   ['MNvT5JekDpg', 5],\n",
       "   ['Sx3Fpw0XCXk', 2],\n",
       "   ['3Q8wacwA4gs', 1],\n",
       "   ['LAtzapS2GBU', 1],\n",
       "   ['n25JjoixM3I', 1]]},\n",
       " {'name': 'python',\n",
       "  'group': 2,\n",
       "  'index': 33,\n",
       "  'conceptCountForEachVid': {'FLZvOKSCkxY': 3},\n",
       "  'count': 3,\n",
       "  'videos_id': [('FLZvOKSCkxY', 3)]},\n",
       " {'name': 'awesome',\n",
       "  'group': 2,\n",
       "  'index': 34,\n",
       "  'conceptCountForEachVid': {'FLZvOKSCkxY': 2,\n",
       "   'MNvT5JekDpg': 3,\n",
       "   'Y90BJzUcqlI': 1},\n",
       "  'count': 6,\n",
       "  'videos_id': [('MNvT5JekDpg', 3), ('FLZvOKSCkxY', 2), ('Y90BJzUcqlI', 1)]}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# YTH\n",
    "# Combine two json\n",
    "filename = '/Users/evelyn/Desktop/ConceptMap/ConceptGuide_code-main/MyResearchCode/vis_json/NLP_test.json'\n",
    "listObj = []\n",
    " \n",
    "# Check if file exists\n",
    "# if path.isfile(filename) is False:\n",
    "#   raise Exception(\"File not found\")\n",
    " \n",
    "# Read JSON file\n",
    "with open(filename) as fp:\n",
    "    listObj = json.load(fp)\n",
    "\n",
    "# # print(listObj['concept_relationship']['nodes'])\n",
    "for i in range(len(review_nodes)):\n",
    "    print(review_nodes[i])\n",
    "# listObj['concept_relationship']['nodes'].append(review_nodes[0])\n",
    "# for i in review_nodes:\n",
    "#     print(i)\n",
    "# print(review_nodes[0])\n",
    "# print(listObj['concept_relationship']['nodes'])\n",
    "# with open(filename, 'w') as json_file:\n",
    "#     json.dump(listObj, json_file, \n",
    "#                         indent=4,  \n",
    "#                         separators=(',',': '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/evelyn/Desktop/ConceptMap/ConceptGuide_code-main/MyResearchCode\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from itertools import combinations, product\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from prerequisite_code import feature3\n",
    "from other_modules import similarity\n",
    "\n",
    "# # import OpenNRE\n",
    "# from OpenNRE import opennre\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'OpenNRE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOpenNRE\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m opennre\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'OpenNRE'"
     ]
    }
   ],
   "source": [
    "from OpenNRE import opennre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_semantic_similarity_videos_wikis( concept_list_dict,  video_resource): \n",
    "\n",
    "    video_transcripts = [video_resource[vid]['transcript'] for vid in video_resource.keys()]\n",
    "    concept_list = list(concept_list_dict.keys())\n",
    "    concept_list_length = len(concept_list)\n",
    "    \n",
    "    count_matrix_video = dok_matrix((concept_list_length, len(video_transcripts)))\n",
    "    for concept_i, transcript_j in product(range(concept_list_length), range(len(video_transcripts))):\n",
    "        if concept_list[concept_i] in video_transcripts[transcript_j]:\n",
    "            #print(concept_list[concept_i], transcript_j)\n",
    "            count_matrix_video[concept_i, transcript_j] = 1\n",
    "\n",
    "    similarity_video = TfidfTransformer(use_idf='True').fit_transform(count_matrix_video)\n",
    "    similarity_video_cos = cosine_similarity(similarity_video, dense_output=False)\n",
    "    ic(similarity_video)\n",
    "    #count_matrix_video_cos = (count_matrix_video.T, dense_output=False)\n",
    "    \n",
    "    \n",
    "    WikiPageRecords = [] \n",
    "    for concept in concept_list:\n",
    "        try:\n",
    "            p = re.sub(r'\\n+|=+', ' ', wikipedia.page(concept).content.lower())\n",
    "        except:\n",
    "            p = ' '\n",
    "        WikiPageRecords.append(p)\n",
    "        \n",
    "    #print(WikiPageRecords)\n",
    "\n",
    "    #count_model = CountVectorizer(vocabulary = concepts_list)\n",
    "    count_matrix_wiki = dok_matrix((concept_list_length, concept_list_length))\n",
    "    # concept_i and concept_j are indexes.\n",
    "    for concept_i, concept_j in combinations(range(concept_list_length), 2): #zip(range(concept_list_length), range(concept_list_length)):\n",
    "        if concept_list[concept_i] in WikiPageRecords[concept_j]:\n",
    "            count_matrix_wiki[concept_i, concept_j] = 1\n",
    "        if concept_list[concept_j] in WikiPageRecords[concept_i]:\n",
    "            count_matrix_wiki[concept_j, concept_i] = 1\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    return WikiPageRecords, similarity_video_cos, count_matrix_wiki #similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiPageRecords, SR_video, SR_wiki_count = concept_semantic_similarity_videos_wikis(existing_name_index_match, original['videos_info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concept relation in sentence level\n",
    "\n",
    "def concept_semantic_similarity_reviews(reviews,  max_ngram_size_cus, concepts_nameindex_match, model='', infer_thres=0.3):\n",
    "    if model=='':\n",
    "        relation_model = opennre.get_model('wiki80_cnn_softmax')\n",
    "    else:\n",
    "        relation_model = model\n",
    "    \n",
    "    concept_list = list(concepts_nameindex_match.keys())\n",
    "    concept_list_length = len(concept_list)\n",
    "    \n",
    "    # part 1: semantic relationship of OpenNRE within sentences.\n",
    "    relation_matrix = dok_matrix((concept_list_length, concept_list_length))\n",
    "    \n",
    "    # part 2: co-occurence within sentences.\n",
    "    relation_count_matrix = dok_matrix((concept_list_length, concept_list_length)) \n",
    "    \n",
    "    sent_bounds = [ [s, s.start,  s.end, s.start_char, s.end_char, []] for s in reviews.sents ]\n",
    "\n",
    "    #unit_vector = []\n",
    "\n",
    "    for chunk in reviews.noun_chunks:\n",
    "        ic(chunk)\n",
    "        if chunk.text.lower() in concepts_nameindex_match:\n",
    "\n",
    "            for sent_content, sent_start, sent_end, sent_start_char, sent_end_char, sent_vector in sent_bounds:\n",
    "                if chunk.start >= sent_start and chunk.end <= sent_end:\n",
    "                    #ic(sent_start, sent_start_char, chunk.start, chunk.end, sent_end, sent_end_char)\n",
    "                    #ic(chunk.start_char -  sent_start_char)\n",
    "                    sent_vector.append(((chunk.start_char-sent_start_char, chunk.end_char-sent_start_char), chunk.text.lower()))\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "     \n",
    "    \n",
    "    relation_list = []\n",
    "    for sent_content, sent_start, sent_end, sent_start_char, sent_end_char, sent_vector in sent_bounds:\n",
    "        for pair_index_1, pair_index_2 in list(combinations(range(len(sent_vector)), 2)):\n",
    "        \n",
    "            kw_1 = sent_vector[pair_index_1][1]\n",
    "            index_1 = sent_vector[pair_index_1][0]\n",
    "            kw_2 = sent_vector[pair_index_2][1]\n",
    "            index_2 = sent_vector[pair_index_2][0]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if kw_1 != kw_2:\n",
    "                \n",
    "                relation_count_matrix[concepts_nameindex_match[kw_1], concepts_nameindex_match[kw_2]] = 1\n",
    "                relation_count_matrix[concepts_nameindex_match[kw_2], concepts_nameindex_match[kw_1]] = 1\n",
    "            \n",
    "                front_infer = relation_model.infer({'text':sent_content.text, 'h': {'pos': (index_1[0], index_1[1])}, 't': {'pos': (index_2[0], index_2[1])}})\n",
    "                back_infer = relation_model.infer({'text':sent_content.text, 'h': {'pos': (index_2[0], index_2[1])}, 't': {'pos': (index_1[0], index_1[1])}})\n",
    "\n",
    "                if front_infer[1]> infer_thres and back_infer[1]> infer_thres:\n",
    "                    relation_matrix[concepts_nameindex_match[kw_1], concepts_nameindex_match[kw_2]] = max(relation_matrix[concepts_nameindex_match[kw_1], concepts_nameindex_match[kw_2]], front_infer[1])\n",
    "                    relation_matrix[concepts_nameindex_match[kw_2], concepts_nameindex_match[kw_1]] = max(relation_matrix[concepts_nameindex_match[kw_2], concepts_nameindex_match[kw_1]], back_infer[1])\n",
    "\n",
    "                elif front_infer[1]> infer_thres:\n",
    "                    relation_matrix[concepts_nameindex_match[kw_1], concepts_nameindex_match[kw_2]] = max(relation_matrix[concepts_nameindex_match[kw_1], concepts_nameindex_match[kw_2]], front_infer[1])\n",
    "\n",
    "                elif back_infer[1]> infer_thres:\n",
    "                    relation_matrix[concepts_nameindex_match[kw_2], concepts_nameindex_match[kw_1]] = max(relation_matrix[concepts_nameindex_match[kw_2], concepts_nameindex_match[kw_1]], back_infer[1]) \n",
    "                    \n",
    "                relation_list.append((kw_1, kw_2, concepts_nameindex_match[kw_1], concepts_nameindex_match[kw_2], front_infer, back_infer))\n",
    "                \n",
    "    \n",
    "    print(relation_matrix)  \n",
    "    print(relation_count_matrix)\n",
    "    return relation_matrix, relation_count_matrix, relation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytextrank\n",
    "\n",
    "nlp_spacy.add_pipe(\"textrank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prerequisite_links( related_nodes, nodes_all_num, video_num, count_matrix_wiki):\n",
    "    \"\"\"\n",
    "    related_nodes: nodes that from reviews\n",
    "    nodes_all_num: node number for old nodes and review nodes.\n",
    "    video_num: for video length.\n",
    "    count_matrix_wiki: count matrix between wikipedia pages of different keywords. \n",
    "    \"\"\"\n",
    "    \n",
    "    #concept_video_count = pd.DataFrame(index=pd.Index(list(concept_list_dict.keys())), columns=pd.Index())\n",
    "    \n",
    "    # Wrr simplier version, updated. with no nan\n",
    "    p = count_matrix_wiki*count_matrix_wiki.T\n",
    "    #q = sparse.diags(1/count_matrix_wiki.sum(axis=1).A.ravel(), 0)\n",
    "    r = count_matrix_wiki.sum(axis=1).A.ravel()\n",
    "    r[r==0]=np.inf\n",
    "    wrw_matrix = sparse.diags(1/r, 0) @ p\n",
    "    wrr_matrix = wrw_matrix - wrw_matrix.T\n",
    "\n",
    "    # old version: wrr_matrix has nan becuase some sum() is 0\n",
    "    #     upper = count_matrix_wiki*count_matrix_wiki.T\n",
    "    #     sparse.diags(1/count_matrix_wiki.sum(axis=1).A.ravel())\n",
    "    #     wrr_matrix = (count_matrix_wiki*count_matrix_wiki.T)/count_matrix_wiki.sum(axis=1)\n",
    "    \n",
    "    \n",
    "    # cld between related_nodes\n",
    "    cld_matrix = dok_matrix((nodes_all_num, nodes_all_num))\n",
    "    \n",
    "    for node_i, node_j in list(combinations(related_nodes, 2)):\n",
    "        cld_matrix[node_i['index'], node_j['index']] = (len(node_i['conceptCountForEachVid']) - len(node_j['conceptCountForEachVid']))/video_num\n",
    "        cld_matrix[node_j['index'], node_i['index']] = (len(node_j['conceptCountForEachVid']) - len(node_i['conceptCountForEachVid']))/video_num\n",
    "        \n",
    "    \n",
    "    return wrr_matrix, cld_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_related_nodes_matrices(related_nodes, text, video_resource, existing_concepts_nameindex_dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    related_nodes: nodes from reviews\n",
    "    text: spacy doc\n",
    "    existing_concepts_nameindex_dict: nodes from old and related nodes.\n",
    "    \"\"\"\n",
    "    #mergednodes = original_nodes.extend(related_nodes)\n",
    "    #mergednodes_ordered = mergednodes #dict(sorted(mergednodes.items(), key=lambda item: item['index']))\n",
    "    #ic(mergednodes_ordered)\n",
    "    \n",
    "    WikiPageRecords, SR_video, SR_wiki_count = concept_semantic_similarity_videos_wikis(existing_concepts_nameindex_dict, video_resource)\n",
    "    #ic(SR_video)\n",
    "    #ic(SR_wiki_count)\n",
    "    SR_review_matix, SR_review_count, SR_review_list = concept_semantic_similarity_reviews(text,  2, existing_concepts_nameindex_dict, infer_thres=0.25)\n",
    "    ic(SR_review_count)\n",
    "    WRR_wiki, CLD_video = prerequisite_links( related_nodes, max(existing_concepts_nameindex_dict.values())+1, len(video_resource), SR_wiki_count)\n",
    "    #ic(WRR_wiki)\n",
    "    #ic(CLD_video)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return SR_video, SR_wiki_count, SR_review_count, WRR_wiki, CLD_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SR_video, SR_wiki_count, SR_review_count, WRR_wiki, CLD_video\n",
    "\n",
    "SR_invideo, SR_inwiki_count, SR_inreview_count, WRR_wiki_01, CLD_video_01 = links_related_nodes_matrices(review_nodes, doc_reviews, original['videos_info'], existing_name_index_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR_invideo.tocoo().row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([SR_invideo.tocoo().row, SR_invideo.tocoo().col, SR_invideo.data]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightedsr = 0.3*SR_invideo + 0.3* SR_inwiki_count+ 0.4*SR_inreview_count\n",
    "weightedpr = WRR_wiki_01 + CLD_video_01\n",
    "\n",
    "\n",
    "\n",
    "test = weightedpr.multiply(weightedsr > 0.3) #multiply(abs(weightedpr) > 0.2).\n",
    "test_values = list(test.todok().values())\n",
    "#plt.hist(test_values)\n",
    "#plt.show()\n",
    "\n",
    "stats.describe(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = weightedsr.multiply(weightedsr > 0.4)\n",
    "test_values = list(test.todok().values())\n",
    "\n",
    "\n",
    "ic(stats.describe(test_values))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_links_similarity_dok  = dok_matrix((33, 33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_display = nx.DiGraph()\n",
    "for node in review_nodes:\n",
    "    g_display.add_node(str(node['index']), label=node['name'], title=str(node['count']), size=3, group=node['group'])\n",
    "    \n",
    "for node in original['concept_relationship']['nodes']:\n",
    "    g_display.add_node(str(node['index']), label=node['name'], title=str(node['count']), size=3, group=node['group'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "for i in original['concept_relationship']['links']:\n",
    "    if i['prerequisite']: \n",
    "        g_display.add_edge(str(i['source']), str(i['target']), weight=weightedsr[i['source'], i['target']]+1)\n",
    "        original_links_similarity_dok[i['source'], i['target']] = weightedsr[i['source'], i['target']]\n",
    "        \n",
    "        #print(str(i['source']), str(i['target']), weightedsr[i['source'], i['target']])\n",
    "\n",
    "    \n",
    "for i in test.todok().items():\n",
    "    #print(existing_index_name_match[i[0][0]], '   ', existing_index_name_match[i[0][1]], '  no: ', i)\n",
    "    \n",
    "    if (str(i[0][0]) in g_display.nodes()) and (str(i[0][1]) in g_display.nodes()):\n",
    "        g_display.add_edge(str(i[0][0]), str(i[0][1]), title = i[1], length=i[1]+1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.describe(list(original_links_similarity_dok.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_nodes[-2]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = Network('600px', '1000px', notebook=True, directed=True)\n",
    "nt.from_nx(g_display)\n",
    "nt.show_buttons(filter_=['nodes', 'edges', 'physics'])\n",
    "nt.show('nx.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_name_index_match\n",
    "existing_index_name_match = {}\n",
    "for i,j in existing_name_index_match.items():\n",
    "    existing_index_name_match[j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = pd.DataFrame(original['concept_relationship']['nodes'] + review_nodes)\n",
    "nodes_df.drop_duplicates(subset=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightedsr = 0.3*SR_invideo + 0.3* SR_inwiki_count+ 0.4*SR_inreview_count\n",
    "weightedpr = WRR_wiki_01 + CLD_video_01\n",
    "\n",
    "\n",
    "\n",
    "test = weightedpr.multiply(weightedsr > 0.3) # when weightedsr > 0.3, what are the corresponding weightedpr\n",
    "test_values = list(test.todok().values())\n",
    "#plt.hist(test_values)\n",
    "#plt.show()\n",
    "\n",
    "stats.describe(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_nodes_ids = [i['index'] for i in review_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list = []\n",
    "\n",
    "for i in original['concept_relationship']['links']:\n",
    "    if i['prerequisite']: \n",
    "        i['weightedsr_review'] = weightedsr[i['source'], i['target']]\n",
    "        i['weightedpr_review'] = weightedpr[i['source'], i['target']]\n",
    "        edges_list.append(i)\n",
    "        #print(str(i['source']), str(i['target']), weightedsr[i['source'], i['target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((weightedsr.multiply(weightedsr > 0.3).multiply(abs(weightedpr) > 0.1))[:,review_nodes_ids][review_nodes_ids,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_selected = (weightedsr.multiply(weightedsr > 0.3).multiply(abs(weightedpr) > 0.1))[:,review_nodes_ids][review_nodes_ids,:]\n",
    "pr_selected = (weightedpr.multiply(weightedsr > 0.3).multiply(abs(weightedpr) > 0.1))[:,review_nodes_ids][review_nodes_ids,:]\n",
    "pr_selected = pr_selected.todok()\n",
    "\n",
    "# upper right triangle and then T\n",
    "# remove duplicate links on pairs\n",
    "pr_triu = sparse.triu(pr_selected).T.todok()\n",
    "\n",
    "for i in pr_triu.items():\n",
    "    \n",
    "    if pr_selected[i[0][0],i[0][1]] == -pr_selected[i[0][1],i[0][0]]:\n",
    "        print(i)\n",
    "        if pr_selected[i[0][0],i[0][1]] < 0:\n",
    "            \n",
    "            pr_selected[i[0][0],i[0][1]] = 0\n",
    "        else:\n",
    "            pr_selected[i[0][1],i[0][0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pr_selected.items():\n",
    "    #print(existing_index_name_match[i[0][0]], '   ', existing_index_name_match[i[0][1]], '  no: ', i)    \n",
    "    if i[1]>0:\n",
    "        edges_list.append({'source': review_nodes_ids[i[0][0]], 'target': review_nodes_ids[i[0][1]], 'weightedsr_review': sr_selected[i[0][0], i[0][1]], 'weightedpr_review': i[1]})\n",
    "    else:\n",
    "        edges_list.append({'source': review_nodes_ids[i[0][1]], 'target': review_nodes_ids[i[0][0]], 'weightedsr_review': sr_selected[i[0][0], i[0][1]], 'weightedpr_review': -i[1]})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pr_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sparse.tril(pr_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pr_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = pd.DataFrame(edges_list)\n",
    "edges_df[['source','target','prerequisite','weightedpr_review', 'similarity','weightedsr_review']].head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df[['prerequisite','weightedpr_review', 'similarity','weightedsr_review']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df.loc[:49,:][['prerequisite','weightedpr_review', 'similarity','weightedsr_review']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df.loc[50:,:][['prerequisite','weightedpr_review', 'similarity','weightedsr_review']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df.loc[:49,:][edges_df['weightedpr_review']<-0.1]#[['prerequisite','weightedpr_review', 'similarity','weightedsr_review']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weightedsr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m original[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcept_relationship\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinks\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprerequisite\u001b[39m\u001b[38;5;124m'\u001b[39m]: \n\u001b[0;32m---> 25\u001b[0m         g_display\u001b[38;5;241m.\u001b[39madd_edge(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m], i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m], weight\u001b[38;5;241m=\u001b[39m\u001b[43mweightedsr\u001b[49m[i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m], i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m         original_links_similarity_dok[i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m], i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m weightedsr[i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m], i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m#print(str(i['source']), str(i['target']), weightedsr[i['source'], i['target']])\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weightedsr' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Set colors by type\n",
    "# carac = carac.set_index('group')\n",
    "# carac = carac.reindex(g_display.nodes())\n",
    " \n",
    "# carac['type'] = pd.Categorical(carac['type'])\n",
    "# carac['type'].cat.codes\n",
    "\n",
    "# Set node size by type\n",
    "#node_sizes = [4500 if entry != 'Person' else 1500 for entry in carac.type]\n",
    "\n",
    "# Set color map\n",
    "#cmap = matplotlib.colors.ListedColormap(['darkorange', 'lightgray', 'dodgerblue'])\n",
    "\n",
    "f = plt.figure(figsize=(10,10))\n",
    "\n",
    "g_display = nx.DiGraph()\n",
    "for node in review_nodes:\n",
    "    g_display.add_node(node['index'], label=node['name'], title=str(node['count']), size=3, group=node['group'])\n",
    "    \n",
    "for node in original['concept_relationship']['nodes']:\n",
    "    g_display.add_node(node['index'], label=node['name'], title=str(node['count']), size=3, group=node['group'])\n",
    "    \n",
    "for i in original['concept_relationship']['links']:\n",
    "    if i['prerequisite']: \n",
    "        g_display.add_edge(i['source'], i['target'], weight=weightedsr[i['source'], i['target']]+1)\n",
    "        original_links_similarity_dok[i['source'], i['target']] = weightedsr[i['source'], i['target']]\n",
    "        \n",
    "        #print(str(i['source']), str(i['target']), weightedsr[i['source'], i['target']])\n",
    "\n",
    "    \n",
    "for i in test.todok().items():\n",
    "    #print(existing_index_name_match[i[0][0]], '   ', existing_index_name_match[i[0][1]], '  no: ', i)\n",
    "    \n",
    "    if (str(i[0][0]) in g_display.nodes()) and (str(i[0][1]) in g_display.nodes()):\n",
    "        g_display.add_edge(i[0][0], i[0][1], title = i[1], length=i[1]+1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Draw the graph and specify our characteristics\n",
    "nx.draw(g_display, with_labels=True, labels=existing_index_name_match, node_color='yellow', \n",
    "        node_size=100, font_size=10, font_weight=\"bold\", width=0.75, \n",
    "        edgecolors='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/zhenv5/breaking_cycles_in_noisy_hierarchies\n",
    "\n",
    "## need to modify python 2.7 to python 3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges_list)\n",
    "edges_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_display = nx.DiGraph()\n",
    "for node in review_nodes:\n",
    "    g_display.add_node(node['index'], label=node['name'], title=str(node['count']), size=3, group=node['group'])\n",
    "    \n",
    "for node in original['concept_relationship']['nodes']:\n",
    "    g_display.add_node(node['index'], label=node['name'], title=str(node['count']), size=3, group=node['group'])\n",
    " \n",
    "\n",
    "for i in edges_list:\n",
    "    g_display.add_edge(i['source'], i['target'], weight=i['weightedsr_review'])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_io import write_to_pickle\n",
    "\n",
    "write_to_pickle(edges_list, 'graph_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_display.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player=nx.pagerank(g_display, alpha = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/Users/jing/ConceptDiscussion/appbackend-conceptdiscussion/breaking_cycles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from remove_cycle_edges_by_hierarchy_greedy import scc_based_to_remove_cycle_edges_iterately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s_c_c import get_big_sccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_big_sccs(g_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = a.pop()\n",
    "nx.is_frozen(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc_based_to_remove_cycle_edges_iterately(g_display,player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_frozen(g_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_display.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
